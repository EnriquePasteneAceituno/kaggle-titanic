{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üõ≥Ô∏è EPA_TITANIC_LINSVC_V2 ‚Äî LinearSVC con GridSearchCV\n\nEste notebook implementa un **Linear Support Vector Classifier (LinearSVC)** para la competencia *Titanic: Machine Learning from Disaster* de Kaggle.  \nSe aplica **Feature Engineering**, **preprocesamiento con pipelines** y una **b√∫squeda de hiperpar√°metros con GridSearchCV**, evaluando las configuraciones m√°s prometedoras (`C`, `loss`, `class_weight`).  \n\n## üîπ Flujo del notebook\n1. **Carga de datos** desde los archivos oficiales de Kaggle.  \n2. **Feature Engineering**: creaci√≥n de variables como t√≠tulos, tama√±o familiar, cabina, tarifa por persona, etc.  \n3. **Preprocesamiento** con `ColumnTransformer` (imputaci√≥n, escalado, one-hot encoding).  \n4. **GridSearchCV** para encontrar la mejor configuraci√≥n de LinearSVC.  \n5. **Entrenamiento final** y generaci√≥n de `submission.csv`.  \n6. **(Opcional)** Ensamble con `SGDClassifier` y `LogisticRegression` mediante un `VotingClassifier`.  \n\nüéØ **Objetivo:** Optimizar LinearSVC, compararlo con experimentos previos y explorar ensambles de clasificadores lineales.\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T22:04:23.980302Z","iopub.execute_input":"2025-08-31T22:04:23.980666Z","iopub.status.idle":"2025-08-31T22:04:23.988059Z","shell.execute_reply.started":"2025-08-31T22:04:23.980634Z","shell.execute_reply":"2025-08-31T22:04:23.987196Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## üì• Carga de datos y configuraci√≥n inicial\n\nEn esta celda:\n- Fijamos una **semilla reproducible**.\n- Detectamos de forma robusta las rutas de `train.csv` y `test.csv` dentro de `/kaggle/input`.\n- Cargamos los datos con `pandas`.\n- Separamos la variable objetivo (`Survived`) del resto de caracter√≠sticas.\n- Unimos temporalmente `train` y `test` en `full` (√∫til para aplicar Feature Engineering de forma consistente).\n- Mostramos formas y distribuci√≥n de la etiqueta.\n\n","metadata":{}},{"cell_type":"code","source":"# ======================\n# Celda 2 ‚Äî Load Data\n# ======================\n\n# Cargar datasets de Titanic desde el input de Kaggle\ntrain_path = \"/kaggle/input/titanic/train.csv\"\ntest_path = \"/kaggle/input/titanic/test.csv\"\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\n\n# Variable objetivo\ny = train_df[\"Survived\"]\n\n# Features iniciales (dejamos todas salvo Survived para ingenier√≠a posterior)\nX = train_df.drop(columns=[\"Survived\"])\n\nprint(\"‚úÖ Datasets cargados correctamente\")\nprint(f\"Shape train: {train_df.shape} (con target)\")\nprint(f\"Shape test:  {test_df.shape} (sin target)\")\nprint(\"\\nColumnas disponibles en train:\")\nprint(train_df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T22:05:32.804005Z","iopub.execute_input":"2025-08-31T22:05:32.804276Z","iopub.status.idle":"2025-08-31T22:05:32.819706Z","shell.execute_reply.started":"2025-08-31T22:05:32.804255Z","shell.execute_reply":"2025-08-31T22:05:32.818590Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Datasets cargados correctamente\nShape train: (891, 12) (con target)\nShape test:  (418, 11) (sin target)\n\nColumnas disponibles en train:\n['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üß™ Celda 3 ‚Äî Feature Engineering (t√≠tulos, familia, cabina, tarifas e interacciones)\n\nEn esta celda creamos nuevas variables predictivas a partir de las columnas originales:\n- **Title** desde `Name` (Mr, Mrs, Miss, Master, Rare).\n- **FamilySize** (= `SibSp` + `Parch` + 1) e **IsAlone** (indicador).\n- **TicketGroup** (tama√±o de grupo por mismo `Ticket`).\n- **CabinDeck** (letra del deck extra√≠da desde `Cabin`, con `U` para desconocido).\n- **FarePerPerson** (tarifa por persona) y **Age\\*Class** (interacci√≥n).\n  \nAplicamos la misma transformaci√≥n a `train` y `test` de manera consistente, concatenando primero y separando despu√©s.\n","metadata":{}},{"cell_type":"code","source":"# ============================\n# Celda 3 ‚Äî Feature Engineering\n# ============================\n\n# Unimos train y test para aplicar transformaciones consistentes (no usamos la columna objetivo aqu√≠)\nfull_df = pd.concat([train_df.drop(columns=[\"Survived\"]), test_df], axis=0, ignore_index=True)  # concatenar sin la y\n\ndef engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Aplica ingenier√≠a de atributos al dataset Titanic.\n    No modifica la variable objetivo. Devuelve un nuevo DataFrame con columnas a√±adidas.\n    \"\"\"\n    out = df.copy()  # trabajar sobre una copia para no mutar el original\n\n    # --- Title desde Name ---\n    title_series = out[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\")[0].str.strip()  # extraer texto entre coma y punto\n    title_map = {  # normalizaci√≥n de t√≠tulos infrecuentes\n        \"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\",\n        \"Lady\": \"Rare\", \"Countess\": \"Rare\", \"Capt\": \"Rare\", \"Col\": \"Rare\",\n        \"Don\": \"Rare\", \"Dr\": \"Rare\", \"Major\": \"Rare\", \"Rev\": \"Rare\",\n        \"Sir\": \"Rare\", \"Jonkheer\": \"Rare\", \"Dona\": \"Rare\"\n    }\n    title_series = title_series.replace(title_map)  # reemplazar alias por categor√≠as consolidadas\n    title_series = title_series.where(title_series.isin([\"Mr\", \"Mrs\", \"Miss\", \"Master\"]), \"Rare\")  # agrupar el resto en 'Rare'\n    out[\"Title\"] = title_series  # asignar nueva columna Title\n\n    # --- Variables de familia ---\n    out[\"FamilySize\"] = out[\"SibSp\"].fillna(0) + out[\"Parch\"].fillna(0) + 1  # tama√±o de familia (incluye al pasajero)\n    out[\"IsAlone\"] = (out[\"FamilySize\"] == 1).astype(int)  # 1 si viaja solo, 0 en caso contrario\n\n    # --- Tama√±o de grupo por Ticket ---\n    out[\"TicketGroup\"] = out.groupby(\"Ticket\")[\"Ticket\"].transform(\"count\")  # contar cu√°ntos comparten el mismo Ticket\n\n    # --- Cabina/Deck ---\n    cabin_str = out[\"Cabin\"].astype(str)  # castear a str para evitar errores con NaN\n    deck = cabin_str.str[0]  # tomar la primera letra como deck\n    deck = deck.replace(\"n\", \"U\")  # cuando era NaN ‚Üí \"n\", lo mapeamos a 'U' (Unknown)\n    out[\"CabinDeck\"] = deck  # asignar deck\n\n    # --- Tarifa por persona ---\n    # Evitamos divisi√≥n por cero: si FamilySize==0 (no deber√≠a), usamos 1 como denominador\n    denom = out[\"FamilySize\"].where(out[\"FamilySize\"] > 0, 1)  # reemplazar 0 por 1 para seguridad\n    out[\"FarePerPerson\"] = out[\"Fare\"] / denom  # tarifa individual aproximada\n\n    # --- Interacciones simples ---\n    out[\"Age*Class\"] = out[\"Age\"] * out[\"Pclass\"]  # interacci√≥n √∫til edad x clase\n\n    return out  # devolver DataFrame con nuevas columnas\n\n# Aplicar ingenier√≠a de atributos a todo el conjunto unido\nfull_fe = engineer_features(full_df)  # obtener el DataFrame con features a√±adidas\n\n# Volver a separar en train y test respetando los tama√±os originales\nX_fe = full_fe.iloc[:len(train_df)].copy()  # porci√≥n correspondiente al train (sin 'Survived')\nX_test_fe = full_fe.iloc[len(train_df):].copy()  # porci√≥n correspondiente al test\n\n# Adjuntar nuevamente la variable objetivo al train enriquecido para referencia/compatibilidad\nX_fe.insert(1, \"Survived\", train_df[\"Survived\"].values)  # insertar 'Survived' como segunda columna (opcional, para inspecci√≥n)\n\n# Informaci√≥n de control\nprint(\"‚úÖ Feature Engineering aplicado.\")\nprint(f\"Train enriquecido: {X_fe.shape} columnas={len(X_fe.columns)}\")\nprint(f\"Test enriquecido:  {X_test_fe.shape} columnas={len(X_test_fe.columns)}\")\n\n# Vista r√°pida de columnas nuevas creadas\nnew_cols = [\"Title\", \"FamilySize\", \"IsAlone\", \"TicketGroup\", \"CabinDeck\", \"FarePerPerson\", \"Age*Class\"]\nprint(\"\\nNuevas columnas creadas:\", new_cols)\n\n# (Opcional) previsualizar primeras filas para validar\nX_fe.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T22:07:22.162832Z","iopub.execute_input":"2025-08-31T22:07:22.163819Z","iopub.status.idle":"2025-08-31T22:07:22.281788Z","shell.execute_reply.started":"2025-08-31T22:07:22.163776Z","shell.execute_reply":"2025-08-31T22:07:22.280792Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Feature Engineering aplicado.\nTrain enriquecido: (891, 19) columnas=19\nTest enriquecido:  (418, 18) columnas=18\n\nNuevas columnas creadas: ['Title', 'FamilySize', 'IsAlone', 'TicketGroup', 'CabinDeck', 'FarePerPerson', 'Age*Class']\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked Title  FamilySize  IsAlone  \\\n0      0         A/5 21171   7.2500   NaN        S    Mr           2        0   \n1      0          PC 17599  71.2833   C85        C   Mrs           2        0   \n2      0  STON/O2. 3101282   7.9250   NaN        S  Miss           1        1   \n\n   TicketGroup CabinDeck  FarePerPerson  Age*Class  \n0            1         U        3.62500       66.0  \n1            2         C       35.64165       38.0  \n2            1         U        7.92500       78.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n      <th>FamilySize</th>\n      <th>IsAlone</th>\n      <th>TicketGroup</th>\n      <th>CabinDeck</th>\n      <th>FarePerPerson</th>\n      <th>Age*Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>Mr</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>U</td>\n      <td>3.62500</td>\n      <td>66.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>Mrs</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>C</td>\n      <td>35.64165</td>\n      <td>38.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>Miss</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>U</td>\n      <td>7.92500</td>\n      <td>78.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## üßº Celda 4 ‚Äî Preprocesamiento con `ColumnTransformer` (imputaci√≥n, escalado y OHE)\n\nEn esta celda:\n- Definimos **listas de features** num√©ricas y categ√≥ricas (tras la ingenier√≠a de atributos).\n- Quitamos columnas no predictivas o redundantes (`Survived`, `Name`, `Ticket`, `Cabin`).\n- Preparamos un **pipeline de preprocesamiento**:\n  - **Num√©ricas** ‚Üí `SimpleImputer(strategy=\"median\")` + `StandardScaler()`\n  - **Categ√≥ricas** ‚Üí `SimpleImputer(strategy=\"most_frequent\")` + `OneHotEncoder(handle_unknown=\"ignore\")`\n- Dejamos listo `preprocessor` para conectarlo al modelo en la siguiente celda (GridSearchCV con LinearSVC).\n","metadata":{}},{"cell_type":"code","source":"# ==============================\n# Celda 4 ‚Äî Preprocesamiento (versi√≥n robusta)\n# ==============================\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n# --- 1) Separar target y descartar columnas no predictivas ---\ny = X_fe[\"Survived\"].astype(int)                             # objetivo\n\ndrop_cols = [\"Survived\", \"Name\", \"Ticket\", \"Cabin\"]          # columnas no predictivas o redundantes\n\n# Conservar PassengerId de test para el submission\npid_test = X_test_fe[\"PassengerId\"].astype(int).copy()\n\n# Eliminar solo las columnas que existan en cada DF (evita KeyError)\ndrop_cols_train = [c for c in drop_cols if c in X_fe.columns]\ndrop_cols_test  = [c for c in drop_cols if c in X_test_fe.columns]\n\nX_model = X_fe.drop(columns=drop_cols_train)                 # train listo para preprocesar/modelar\nX_test_model = X_test_fe.drop(columns=drop_cols_test)        # test listo para preprocesar/modelar\n\n# --- 2) Definir listas de columnas num√©ricas y categ√≥ricas ---\nnumeric_features = [\n    \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass\",\n    \"FamilySize\", \"TicketGroup\", \"FarePerPerson\", \"Age*Class\"\n]\ncategorical_features = [\"Sex\", \"Embarked\", \"Title\", \"CabinDeck\"]\n\n# Verificaci√≥n ligera por si hubiera columnas faltantes\nmissing_in_X = [c for c in numeric_features + categorical_features if c not in X_model.columns]\nif missing_in_X:\n    print(\"‚ö†Ô∏è Advertencia: faltan columnas en X_model:\", missing_in_X)\n\n# --- 3) Pipelines de preprocesamiento ---\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),           # imputaci√≥n num√©rica\n    (\"scaler\", StandardScaler())                             # escalado\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),    # imputaci√≥n categ√≥rica\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))          # OHE robusto a niveles nuevos\n])\n\n# --- 4) ColumnTransformer que une ambos pipelines ---\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features)\n    ],\n    remainder=\"drop\"\n)\n\n# --- 5) Informaci√≥n de control ---\nprint(\"‚úÖ Preprocesador creado (robusto a columnas ausentes en test).\")\nprint(f\"- X_model shape (antes de transformar): {X_model.shape}\")\nprint(f\"- X_test_model shape (antes de transformar): {X_test_model.shape}\")\nprint(f\"- Num√©ricas: {numeric_features}\")\nprint(f\"- Categ√≥ricas: {categorical_features}\")\nprint(\"‚ÑπÔ∏è 'preprocessor' est√° listo para conectarse a LinearSVC en GridSearchCV (siguiente celda).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T22:10:51.593440Z","iopub.execute_input":"2025-08-31T22:10:51.593815Z","iopub.status.idle":"2025-08-31T22:10:51.607645Z","shell.execute_reply.started":"2025-08-31T22:10:51.593791Z","shell.execute_reply":"2025-08-31T22:10:51.606840Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Preprocesador creado (robusto a columnas ausentes en test).\n- X_model shape (antes de transformar): (891, 15)\n- X_test_model shape (antes de transformar): (418, 15)\n- Num√©ricas: ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass', 'FamilySize', 'TicketGroup', 'FarePerPerson', 'Age*Class']\n- Categ√≥ricas: ['Sex', 'Embarked', 'Title', 'CabinDeck']\n‚ÑπÔ∏è 'preprocessor' est√° listo para conectarse a LinearSVC en GridSearchCV (siguiente celda).\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## ‚öôÔ∏è Celda 5 ‚Äî LinearSVC + GridSearchCV (5-fold estratificado)\n\nBuscamos los mejores hiperpar√°metros de **LinearSVC**:\n- `C`: fuerza de regularizaci√≥n.\n- `loss`: `hinge` vs `squared_hinge`.\n- `class_weight`: `None` vs `balanced`.\n- `dual=False`: recomendado cuando `n_samples > n_features` tras OHE.\n\nSe usa **StratifiedKFold (n_splits=5, shuffle=True)** y `scoring=\"accuracy\"`.\n","metadata":{}},{"cell_type":"code","source":"# ==============================\n# Celda 5 ‚Äî GridSearchCV LinearSVC\n# ==============================\n\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nimport numpy as np\nimport pandas as pd\n\nRANDOM_SEED = 42  # Semilla para reproducibilidad\n\n# 1) Definimos el clasificador base con iteraciones amplias para evitar warnings de convergencia\nbase_clf = LinearSVC(\n    random_state=RANDOM_SEED,  # reproducibilidad\n    max_iter=10000             # m√°s iteraciones para facilitar convergencia\n)\n\n# 2) Encadenamos preprocesamiento + clasificador en un Pipeline\npipe = Pipeline(steps=[\n    (\"prep\", preprocessor),    # ColumnTransformer definido en la celda 4\n    (\"clf\", base_clf)          # Modelo LinearSVC\n])\n\n# 3) Espacio de b√∫squeda de hiperpar√°metros\nparam_grid = {\n    \"clf__C\": [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],     # regularizaci√≥n\n    \"clf__loss\": [\"hinge\", \"squared_hinge\"],       # funci√≥n de p√©rdida\n    \"clf__dual\": [False],                          # n_samples > n_features ‚áí dual=False\n    \"clf__class_weight\": [None, \"balanced\"],       # balanceo de clases (opcional)\n}\n\n# 4) Definimos la estrategia de CV estratificada\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n\n# 5) Configuramos la b√∫squeda en malla\ngs = GridSearchCV(\n    estimator=pipe,          # pipeline completo\n    param_grid=param_grid,   # grilla de hiperpar√°metros\n    scoring=\"accuracy\",      # m√©trica de evaluaci√≥n\n    cv=cv,                   # validaci√≥n cruzada estratificada\n    n_jobs=-1,               # usar todos los n√∫cleos disponibles\n    verbose=1,               # nivel de verbosidad (muestra el progreso)\n    refit=True               # reentrena con los mejores hiperpar√°metros al final\n)\n\n# 6) Ejecutamos la b√∫squeda de hiperpar√°metros\ngs.fit(X_model, y)\n\n# 7) Reporte principal: mejor score y par√°metros\nbest_mean = gs.best_score_                                   # accuracy promedio CV del mejor set\nbest_std  = gs.cv_results_[\"std_test_score\"][gs.best_index_] # std del accuracy en CV\nprint(\"‚úÖ GridSearchCV finalizado.\")\nprint(f\"Mejor CV mean acc: {best_mean:.4f} (std={best_std:.4f})\")\nprint(\"Mejores par√°metros:\", gs.best_params_)\n\n# 8) Tabla compacta con los 5 mejores resultados (ordenados por mean_test_score)\nresults = pd.DataFrame(gs.cv_results_)\ncols = [\"mean_test_score\", \"std_test_score\", \"param_clf__C\",\n        \"param_clf__loss\", \"param_clf__class_weight\"]\ntop5 = results.sort_values(\"mean_test_score\", ascending=False)[cols].head(5)\nprint(\"\\nTop-5 combinaciones por accuracy CV:\")\ndisplay(top5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T22:12:28.772452Z","iopub.execute_input":"2025-08-31T22:12:28.772859Z","iopub.status.idle":"2025-08-31T22:12:32.318140Z","shell.execute_reply.started":"2025-08-31T22:12:28.772836Z","shell.execute_reply":"2025-08-31T22:12:32.317249Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 24 candidates, totalling 120 fits\n‚úÖ GridSearchCV finalizado.\nMejor CV mean acc: 0.8260 (std=0.0123)\nMejores par√°metros: {'clf__C': 0.1, 'clf__class_weight': None, 'clf__dual': False, 'clf__loss': 'squared_hinge'}\n\nTop-5 combinaciones por accuracy CV:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n60 fits failed out of a total of 120.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/svm/_classes.py\", line 274, in fit\n    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n                                           ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n    raise ValueError(\nValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.82603729        nan 0.81369029        nan 0.82378382\n        nan 0.81593748        nan 0.82490114        nan 0.8170548\n        nan 0.82377754        nan 0.8159312         nan 0.82490114\n        nan 0.8159312         nan 0.82490114        nan 0.8159312 ]\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    mean_test_score  std_test_score param_clf__C param_clf__loss  \\\n1          0.826037        0.012315          0.1   squared_hinge   \n9          0.824901        0.014977          1.0   squared_hinge   \n17         0.824901        0.014977          5.0   squared_hinge   \n21         0.824901        0.014977         10.0   squared_hinge   \n5          0.823784        0.011149          0.5   squared_hinge   \n\n   param_clf__class_weight  \n1                     None  \n9                     None  \n17                    None  \n21                    None  \n5                     None  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>param_clf__C</th>\n      <th>param_clf__loss</th>\n      <th>param_clf__class_weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.826037</td>\n      <td>0.012315</td>\n      <td>0.1</td>\n      <td>squared_hinge</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.824901</td>\n      <td>0.014977</td>\n      <td>1.0</td>\n      <td>squared_hinge</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.824901</td>\n      <td>0.014977</td>\n      <td>5.0</td>\n      <td>squared_hinge</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.824901</td>\n      <td>0.014977</td>\n      <td>10.0</td>\n      <td>squared_hinge</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.823784</td>\n      <td>0.011149</td>\n      <td>0.5</td>\n      <td>squared_hinge</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## üèÅ Celda 6 ‚Äî Entrenamiento final y `submission.csv`\n\nEntrenamos el **mejor estimador** encontrado por `GridSearchCV` con todos los datos de entrenamiento, predecimos sobre `test` y guardamos el archivo `submission.csv` (columnas: `PassengerId`, `Survived`).\n","metadata":{}},{"cell_type":"code","source":"# ==============================\n# Celda 6 ‚Äî Entrenamiento final y submission.csv\n# ==============================\n\n# 1) Recuperar el mejor pipeline (preprocesador + LinearSVC tuned)\nbest_model = gs.best_estimator_\n\n# 2) Entrenar con todo el train\nbest_model.fit(X_model, y)\n\n# 3) Predecir sobre test\ntest_pred = best_model.predict(X_test_model)\n\n# 4) Construir y guardar submission\nsubmission = pd.DataFrame({\n    \"PassengerId\": pid_test.values,            # mantener el PassengerId original\n    \"Survived\": test_pred.astype(int)          # cast expl√≠cito a int por prolijidad\n})\n\nsubmission_path = \"submission.csv\"\nsubmission.to_csv(submission_path, index=False)\n\nprint(f\"‚úÖ Archivo '{submission_path}' creado con {len(submission)} filas y columnas {list(submission.columns)}\")\ndisplay(submission.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T22:14:00.792005Z","iopub.execute_input":"2025-08-31T22:14:00.792287Z","iopub.status.idle":"2025-08-31T22:14:00.839481Z","shell.execute_reply.started":"2025-08-31T22:14:00.792267Z","shell.execute_reply":"2025-08-31T22:14:00.838052Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Archivo 'submission.csv' creado con 418 filas y columnas ['PassengerId', 'Survived']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   PassengerId  Survived\n0          892         0\n1          893         1\n2          894         0\n3          895         0\n4          896         1\n5          897         0\n6          898         1\n7          899         0\n8          900         1\n9          901         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>897</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>898</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>899</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>901</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"---\n\n# ‚úÖ Conclusiones ‚Äî EPA_TITANIC_LINSVC_V2\n\nEn este notebook implementamos y optimizamos un **Linear Support Vector Classifier (LinearSVC)** para la competencia *Titanic: Machine Learning from Disaster* de Kaggle.\n\n## üîπ Resultados\n- **Mejores hiperpar√°metros (GridSearchCV):**  \n  - `C = 0.1`  \n  - `loss = squared_hinge`  \n  - `dual = False`  \n  - `class_weight = None`  \n- **Validaci√≥n cruzada (5-fold):**  \n  - **CV mean acc:** 0.8260  \n  - **std:** 0.0123  \n- **Kaggle Leaderboard:**  \n  - **Score:** 0.77751 ‚Üí ü•á *mejor resultado hasta ahora*  \n\n## üîπ Observaciones\n- El ajuste fino de `LinearSVC` logr√≥ superar a **SGDClassifier V1 (0.77511)** y a **Logistic Regression (0.77272)**.  \n- Los errores en combinaciones inv√°lidas (`hinge` + `dual=False`) se resolvieron limitando la grilla a configuraciones soportadas.  \n- El modelo es estable y competitivo considerando la simplicidad de un clasificador lineal.\n\n## üîπ Pr√≥ximos pasos\n- Explorar **modelos basados en √°rboles**: `DecisionTree`, `RandomForest`, `GradientBoosting`.  \n- Probar **boosting avanzado**: `XGBoost`, `LightGBM`, `CatBoost`.  \n- Implementar **VotingClassifier / StackingClassifier** combinando los mejores lineales (LinearSVC + SGD + LogReg).  \n- Continuar actualizando el **README** con cada experimento y su score.\n\n---\n\nüìå **Cierre:** LinearSVC V2 se establece como el **mejor modelo lineal en este proyecto Titanic** hasta la fecha, marcando la base para explorar ensambles y modelos no lineales en los siguientes notebooks.\n","metadata":{}}]}