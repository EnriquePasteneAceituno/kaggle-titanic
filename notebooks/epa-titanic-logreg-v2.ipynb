{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üö¢ EPA Titanic ‚Äî Logistic Regression V2 (GridSearch)\nAjustamos hiperpar√°metros de **Regresi√≥n Log√≠stica** (`C`, `penalty`, `solver`) con **GridSearchCV**.\nCompararemos contra V1 para intentar mejorar el score en Kaggle.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:02:57.182039Z","iopub.execute_input":"2025-08-29T16:02:57.182320Z","iopub.status.idle":"2025-08-29T16:02:59.756922Z","shell.execute_reply.started":"2025-08-29T16:02:57.182297Z","shell.execute_reply":"2025-08-29T16:02:59.755643Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# üì• Celda 2 ‚Äî Carga de datos\nLeemos los archivos `train.csv` y `test.csv` desde el directorio `/kaggle/input/titanic/`.  \n- `train.csv` contiene 891 pasajeros con la variable objetivo `Survived`.  \n- `test.csv` contiene 418 pasajeros sin `Survived`, que usaremos para generar las predicciones finales.  \nMostramos las dimensiones y una vista previa de las primeras filas para confirmar que todo est√° correcto.\n","metadata":{}},{"cell_type":"code","source":"# üì• Celda 2 ‚Äî Carga de datos (train y test)\n\nimport pandas as pd  # librer√≠a para manejo de datos en tablas (DataFrames)\n\n# Leemos los archivos oficiales provistos por Kaggle en el entorno de la competici√≥n\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")  # dataset de entrenamiento (incluye Survived)\ntest_df  = pd.read_csv(\"/kaggle/input/titanic/test.csv\")   # dataset de test (sin Survived)\n\n# Mostramos dimensiones para validar tama√±os esperados\nprint(\"Shape train:\", train_df.shape)  # deber√≠a ser (891, 12)\nprint(\"Shape test :\", test_df.shape)   # deber√≠a ser (418, 11)\n\n# Vista r√°pida de las primeras filas para confirmar columnas y tipos\ntrain_df.head()  # muestra las 5 primeras filas\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:07:43.337992Z","iopub.execute_input":"2025-08-29T16:07:43.338627Z","iopub.status.idle":"2025-08-29T16:07:43.398321Z","shell.execute_reply.started":"2025-08-29T16:07:43.338599Z","shell.execute_reply":"2025-08-29T16:07:43.397371Z"}},"outputs":[{"name":"stdout","text":"Shape train: (891, 12)\nShape test : (418, 11)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# üîé Celda 3 ‚Äî EDA breve (faltantes y balance de Survived)\nRevisamos valores faltantes en `train` y `test`, y verificamos el balance de clases en `Survived`.\nEsto nos permitir√° definir qu√© imputaciones ser√°n necesarias antes de entrenar el modelo.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np  # librer√≠a para operaciones num√©ricas\n\n# --- Faltantes en TRAIN ---\nna_train = train_df.isna().sum()                                # cuenta valores NaN por columna\nna_train_pct = (na_train / len(train_df)).round(3)              # calcula % de NaN\nfaltantes_train = pd.DataFrame({\"nulos\": na_train, \"porcentaje\": na_train_pct})\nprint(\"Faltantes en train (conteo y %):\")\nprint(faltantes_train.sort_values(\"nulos\", ascending=False))    # ordena de mayor a menor\n\n# --- Faltantes en TEST ---\nna_test = test_df.isna().sum()\nna_test_pct = (na_test / len(test_df)).round(3)\nfaltantes_test = pd.DataFrame({\"nulos\": na_test, \"porcentaje\": na_test_pct})\nprint(\"\\nFaltantes en test (conteo y %):\")\nprint(faltantes_test.sort_values(\"nulos\", ascending=False))\n\n# --- Distribuci√≥n de Survived ---\ny_counts = train_df[\"Survived\"].value_counts().sort_index()     # conteo de 0 y 1\ny_ratio = train_df[\"Survived\"].value_counts(normalize=True).sort_index().round(4)  # proporciones\n\nprint(\"\\nDistribuci√≥n de Survived (conteo):\")\nprint(y_counts)\n\nprint(\"\\nDistribuci√≥n de Survived (proporci√≥n):\")\nprint(y_ratio)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:10:11.277449Z","iopub.execute_input":"2025-08-29T16:10:11.277785Z","iopub.status.idle":"2025-08-29T16:10:11.326909Z","shell.execute_reply.started":"2025-08-29T16:10:11.277763Z","shell.execute_reply":"2025-08-29T16:10:11.325926Z"}},"outputs":[{"name":"stdout","text":"Faltantes en train (conteo y %):\n             nulos  porcentaje\nCabin          687       0.771\nAge            177       0.199\nEmbarked         2       0.002\nPassengerId      0       0.000\nSurvived         0       0.000\nPclass           0       0.000\nName             0       0.000\nSex              0       0.000\nSibSp            0       0.000\nParch            0       0.000\nTicket           0       0.000\nFare             0       0.000\n\nFaltantes en test (conteo y %):\n             nulos  porcentaje\nCabin          327       0.782\nAge             86       0.206\nFare             1       0.002\nPassengerId      0       0.000\nPclass           0       0.000\nName             0       0.000\nSex              0       0.000\nSibSp            0       0.000\nParch            0       0.000\nTicket           0       0.000\nEmbarked         0       0.000\n\nDistribuci√≥n de Survived (conteo):\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nDistribuci√≥n de Survived (proporci√≥n):\nSurvived\n0    0.6162\n1    0.3838\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# üß± Celda 4 ‚Äî Definici√≥n de variables base y objetivo\nSeparamos las columnas que usaremos como **features base** (`X` y `X_test`) y el **target** (`y`).  \nAdem√°s, guardamos `PassengerId` del test para poder generar el archivo de `submission.csv` al final.","metadata":{}},{"cell_type":"code","source":"# --- Definici√≥n de columnas base ---\n# Num√©ricas originales del dataset\nbase_num_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass\"]\n\n# Categ√≥ricas originales del dataset\nbase_cat_features = [\"Sex\", \"Embarked\"]\n\n# --- Construcci√≥n de X (features de entrenamiento) ---\nX = train_df[base_num_features + base_cat_features].copy()  # selecciona columnas base de train\n\n# --- Target (variable objetivo) ---\ny = train_df[\"Survived\"].astype(int)  # convertimos Survived a entero (0/1)\n\n# --- Construcci√≥n de X_test (features de predicci√≥n) ---\nX_test = test_df[base_num_features + base_cat_features].copy()  # selecciona columnas base de test\n\n# --- Guardamos PassengerId (para submission) ---\ntest_passenger_id = test_df[\"PassengerId\"].copy()  # IDs del test\n\n# Vista previa de las primeras filas de X\nX.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:15:35.424217Z","iopub.execute_input":"2025-08-29T16:15:35.424547Z","iopub.status.idle":"2025-08-29T16:15:35.443598Z","shell.execute_reply.started":"2025-08-29T16:15:35.424521Z","shell.execute_reply":"2025-08-29T16:15:35.442492Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    Age  SibSp  Parch     Fare  Pclass     Sex Embarked\n0  22.0      1      0   7.2500       3    male        S\n1  38.0      1      0  71.2833       1  female        C\n2  26.0      0      0   7.9250       3  female        S\n3  35.0      1      0  53.1000       1  female        S\n4  35.0      0      0   8.0500       3    male        S","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>3</td>\n      <td>male</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>female</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>3</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>1</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>3</td>\n      <td>male</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# üß™ Celda 5 ‚Äî Ingenier√≠a de atributos (para LogReg V2)\nCreamos nuevas variables dentro de una funci√≥n que luego integraremos al `Pipeline` v√≠a `FunctionTransformer`:\n\n- `FamilySize = SibSp + Parch + 1`  \n- `IsAlone = 1 si FamilySize == 1, si no 0`  \n- `Title` (extra√≠do desde `Name`, normalizando y agrupando raros en `Other`)  \n- `CabinKnown = 1 si Cabin no es NaN, si no 0`  \n- `FarePerPerson = Fare / FamilySize`  \n- `TicketGroupSize = cantidad de pasajeros que comparten el mismo Ticket`  \n\n> Nota: Para poder extraer `Title` y calcular `TicketGroupSize`/`CabinKnown`, a√±adimos temporalmente las columnas auxiliares `Name`, `Ticket` y `Cabin` a `X`/`X_test`.  \n","metadata":{}},{"cell_type":"code","source":"# Importamos el transformador funcional para enchufar la FE al pipeline\nfrom sklearn.preprocessing import FunctionTransformer  # permite aplicar una funci√≥n custom a X dentro del pipeline\nimport pandas as pd                                    # por si lo necesitamos para Series/DF\nimport numpy as np                                     # utilidades num√©ricas\n\n# Definimos la funci√≥n de ingenier√≠a de atributos\ndef add_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()  # trabajamos sobre una copia para no modificar el objeto original\n\n    # --- FamilySize e IsAlone ---\n    df[\"FamilySize\"] = df[\"SibSp\"].fillna(0) + df[\"Parch\"].fillna(0) + 1  # suma familiares + el pasajero\n    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)                   # 1 si viaja solo, 0 en caso contrario\n\n    # --- Title (extra√≠do de Name) ---\n    if \"Name\" in df.columns:                                              # nos aseguramos de tener la columna\n        titles = df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\")[0]             # t√≠tulo entre coma y punto\n        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})  # normalizamos variantes\n        # nos quedamos con los m√°s frecuentes y agrupamos el resto como 'Other'\n        common = [\"Mr\", \"Mrs\", \"Miss\", \"Master\"]\n        titles = titles.where(titles.isin(common), \"Other\")\n        df[\"Title\"] = titles.fillna(\"Other\")                               # rellenamos posibles NaN como 'Other'\n    else:\n        df[\"Title\"] = \"Other\"                                              # fallback si no est√° Name (no deber√≠a pasar)\n\n    # --- CabinKnown (bandera si Cabin est√° presente) ---\n    if \"Cabin\" in df.columns:\n        df[\"CabinKnown\"] = (~df[\"Cabin\"].isna()).astype(int)               # 1 si Cabin no es NaN, 0 si lo es\n    else:\n        df[\"CabinKnown\"] = 0                                               # fallback si no est√° la columna\n\n    # --- FarePerPerson (evitar div. por cero con replace(0,1)) ---\n    df[\"FarePerPerson\"] = df[\"Fare\"].fillna(df[\"Fare\"].median()) / df[\"FamilySize\"].replace(0, 1)\n\n    # --- TicketGroupSize (tama√±o del grupo por ticket) ---\n    if \"Ticket\" in df.columns:\n        counts = df[\"Ticket\"].map(df[\"Ticket\"].value_counts())             # mapea frecuencia de cada ticket\n        df[\"TicketGroupSize\"] = counts.fillna(1).astype(int)               # si no hay match, asumimos 1\n    else:\n        df[\"TicketGroupSize\"] = 1                                          # fallback\n\n    return df  # devolvemos el DF con las nuevas columnas\n\n# Creamos el transformador para integrarlo al Pipeline en celdas posteriores\nfeature_engineering = FunctionTransformer(add_features, validate=False)  # validate=False para permitir DataFrame\n\n# A√±adimos columnas auxiliares (Name/Ticket/Cabin) a X y X_test para que FE funcione correctamente\nfor col in [\"Name\", \"Ticket\", \"Cabin\"]:\n    if col not in X.columns and col in train_df.columns:  # si no est√° en X pero s√≠ en train_df, la agregamos alineada por √≠ndice\n        X[col] = train_df.loc[X.index, col]\n    if col not in X_test.columns and col in test_df.columns:  # idem para test\n        X_test[col] = test_df.loc[X_test.index, col]\n\nprint(\"‚úÖ Ingenier√≠a de atributos lista (FunctionTransformer definido y columnas auxiliares a√±adidas).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:19:52.862195Z","iopub.execute_input":"2025-08-29T16:19:52.862613Z","iopub.status.idle":"2025-08-29T16:19:53.452103Z","shell.execute_reply.started":"2025-08-29T16:19:52.862587Z","shell.execute_reply":"2025-08-29T16:19:53.451140Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Ingenier√≠a de atributos lista (FunctionTransformer definido y columnas auxiliares a√±adidas).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# üßΩ Celda 6 ‚Äî Preprocesamiento (imputaci√≥n, escalado y One-Hot)\nDefinimos un `ColumnTransformer` que:\n- Imputa **num√©ricas** con **mediana** y luego **escala** (beneficia a LogReg).\n- Imputa **categ√≥ricas** con **moda** y aplica **One-Hot** (incluye `Title` creado en FE).\n","metadata":{}},{"cell_type":"code","source":"# Importamos utilidades para construir el preprocesamiento por tipo de columna\nfrom sklearn.compose import ColumnTransformer                 # aplica transformadores por columna\nfrom sklearn.pipeline import Pipeline                         # encadena pasos (FE -> pre -> modelo)\nfrom sklearn.impute import SimpleImputer                      # imputaci√≥n de valores faltantes\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder  # escalado y one-hot\n\n# Definimos las columnas que existir√°n DESPU√âS del paso de Feature Engineering (Celda 5)  # <- comentario\nnum_features = [                                              # lista de columnas num√©ricas finales\n    \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass\",               # num√©ricas originales\n    \"FamilySize\", \"IsAlone\", \"FarePerPerson\",                # num√©ricas creadas\n    \"TicketGroupSize\", \"CabinKnown\"                          # num√©ricas/bool creadas\n]\ncat_features = [                                              # lista de columnas categ√≥ricas finales\n    \"Sex\", \"Embarked\", \"Title\"                               # categ√≥ricas (Title viene de FE)\n]\n\n# Pipeline num√©rico: imputar con mediana y escalar a media 0, std 1\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),           # rellena NaN en num√©ricas con la mediana\n    (\"scaler\", StandardScaler())                             # estandariza num√©ricas (cr√≠tico para LogReg)\n])\n\n# Pipeline categ√≥rico: imputar con moda y codificar en dummies densos\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),    # rellena NaN en categ√≥ricas con la moda\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\",        # crea columnas dummy, ignora categor√≠as nuevas\n                             sparse_output=False))           # salida densa (array numpy)\n])\n\n# Componemos el ColumnTransformer aplicando cada pipeline a su grupo de columnas\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, num_features),          # aplica pipeline num√©rico a num_features\n        (\"cat\", categorical_transformer, cat_features)       # aplica pipeline categ√≥rico a cat_features\n    ],\n    remainder=\"drop\"                                         # descarta columnas no listadas (p. ej. Name/Ticket/Cabin)\n)\n\n# Mensajes de control para verificar la configuraci√≥n\nprint(\"‚úÖ Preprocesamiento definido.\")\nprint(\"Num features:\", num_features)\nprint(\"Cat features:\", cat_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:24:58.708974Z","iopub.execute_input":"2025-08-29T16:24:58.709920Z","iopub.status.idle":"2025-08-29T16:24:58.718938Z","shell.execute_reply.started":"2025-08-29T16:24:58.709889Z","shell.execute_reply":"2025-08-29T16:24:58.717947Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Preprocesamiento definido.\nNum features: ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass', 'FamilySize', 'IsAlone', 'FarePerPerson', 'TicketGroupSize', 'CabinKnown']\nCat features: ['Sex', 'Embarked', 'Title']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# üîé Celda 7 ‚Äî GridSearchCV para Regresi√≥n Log√≠stica\nBuscamos la mejor combinaci√≥n de hiperpar√°metros para **LogisticRegression**:\n\n- `C` (fuerza de regularizaci√≥n, menor = m√°s regularizaci√≥n)  \n- `penalty` (`l1` o `l2`)  \n- `solver` (compatibles con cada `penalty`)  \n\n> Notas:\n> - `lbfgs` ‚Üí solo `l2`  \n> - `liblinear` ‚Üí `l1` o `l2` (binario)  \n> - `saga` ‚Üí `l1` o `l2` (escala mejor con muchas features)  \n","metadata":{}},{"cell_type":"code","source":"# Importamos el modelo y utilidades de b√∫squeda\nfrom sklearn.linear_model import LogisticRegression                  # regresi√≥n log√≠stica\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV    # CV y Grid Search\nfrom sklearn.pipeline import Pipeline                                # para encadenar pasos\nimport numpy as np                                                   # operaciones num√©ricas\nimport pandas as pd                                                  # para mostrar resultados ordenados\n\n# Definimos el \"estimator\" base (los hiperpar√°metros los variar√° el grid)\nlogreg_base = LogisticRegression(max_iter=5000)  # iteraciones altas para asegurar convergencia\n\n# Pipeline completo: FE -> Preprocesamiento -> Modelo\npipe_logreg = Pipeline(steps=[\n    (\"fe\", feature_engineering),   # crea features nuevas (Celda 5)\n    (\"pre\", preprocessor),         # imputaci√≥n + escalado + one-hot (Celda 6)\n    (\"logreg\", logreg_base)        # regresi√≥n log√≠stica\n])\n\n# Definimos una rejilla de hiperpar√°metros compatible solver/penalty\n# Usamos varios valores de C en escala logar√≠tmica (m√°s fino alrededor de 0.1‚Äì10)\nC_values = np.logspace(-3, 2, 10)   # 0.001 ... 100\n\nparam_grid = [\n    # lbfgs con l2\n    {\n        \"logreg__solver\": [\"lbfgs\"],\n        \"logreg__penalty\": [\"l2\"],\n        \"logreg__C\": C_values\n    },\n    # liblinear con l1 o l2\n    {\n        \"logreg__solver\": [\"liblinear\"],\n        \"logreg__penalty\": [\"l1\", \"l2\"],\n        \"logreg__C\": C_values\n    },\n    # saga con l1 o l2 (robusto con muchas columnas despu√©s del one-hot)\n    {\n        \"logreg__solver\": [\"saga\"],\n        \"logreg__penalty\": [\"l1\", \"l2\"],\n        \"logreg__C\": C_values\n    }\n]\n\n# Esquema de validaci√≥n cruzada estratificada\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Configuramos el GridSearchCV\ngrid = GridSearchCV(\n    estimator=pipe_logreg,         # pipeline completo\n    param_grid=param_grid,         # combinaciones de hiperpar√°metros\n    scoring=\"accuracy\",            # m√©trica oficial\n    cv=cv,                         # validaci√≥n cruzada\n    n_jobs=-1,                     # usa todos los cores disponibles\n    refit=True,                    # reentrena el mejor modelo en todo el train\n    verbose=1                      # verboso para seguimiento\n)\n\n# Ejecutamos la b√∫squeda\ngrid.fit(X, y)\n\n# Reportamos el mejor resultado y los mejores hiperpar√°metros\nprint(\"üîù Mejor accuracy (CV):\", grid.best_score_.round(4))\nprint(\"üîß Mejores hiperpar√°metros:\", grid.best_params_)\n\n# Guardamos el mejor pipeline ya reentrenado (listo para predecir)\nbest_model = grid.best_estimator_\n\n# (Opcional) Mostrar el Top-5 de combinaciones\nres = pd.DataFrame(grid.cv_results_).sort_values(\"mean_test_score\", ascending=False)\ncols = [\"mean_test_score\",\"std_test_score\",\"param_logreg__solver\",\"param_logreg__penalty\",\"param_logreg__C\"]\ndisplay(res[cols].head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:39:57.480167Z","iopub.execute_input":"2025-08-29T16:39:57.481256Z","iopub.status.idle":"2025-08-29T16:40:12.939911Z","shell.execute_reply.started":"2025-08-29T16:39:57.481224Z","shell.execute_reply":"2025-08-29T16:40:12.937445Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 50 candidates, totalling 250 fits\nüîù Mejor accuracy (CV): 0.8272\nüîß Mejores hiperpar√°metros: {'logreg__C': 7.742636826811277, 'logreg__penalty': 'l2', 'logreg__solver': 'lbfgs'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    mean_test_score  std_test_score param_logreg__solver  \\\n25         0.827155        0.010978            liblinear   \n45         0.827155        0.010978                 saga   \n24         0.827155        0.010978            liblinear   \n22         0.827155        0.010978            liblinear   \n7          0.827155        0.010978                lbfgs   \n\n   param_logreg__penalty param_logreg__C  \n25                    l2        7.742637  \n45                    l2        7.742637  \n24                    l1        7.742637  \n22                    l1        2.154435  \n7                     l2        7.742637  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>param_logreg__solver</th>\n      <th>param_logreg__penalty</th>\n      <th>param_logreg__C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25</th>\n      <td>0.827155</td>\n      <td>0.010978</td>\n      <td>liblinear</td>\n      <td>l2</td>\n      <td>7.742637</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.827155</td>\n      <td>0.010978</td>\n      <td>saga</td>\n      <td>l2</td>\n      <td>7.742637</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.827155</td>\n      <td>0.010978</td>\n      <td>liblinear</td>\n      <td>l1</td>\n      <td>7.742637</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.827155</td>\n      <td>0.010978</td>\n      <td>liblinear</td>\n      <td>l1</td>\n      <td>2.154435</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.827155</td>\n      <td>0.010978</td>\n      <td>lbfgs</td>\n      <td>l2</td>\n      <td>7.742637</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# üì§ Celda 8 ‚Äî Predicci√≥n final y creaci√≥n de `submission.csv`\nUsamos el **mejor pipeline del GridSearch** para predecir en `test` y\nguardamos el archivo en el formato requerido por Kaggle.\n","metadata":{}},{"cell_type":"code","source":"# 1) Usamos el mejor estimador encontrado por GridSearch (ya est√° ajustado con refit=True)\n#    Si cerraste el kernel, re-ejecuta la Celda 7 antes para tener `best_model` disponible.\n\n# 2) Predicciones sobre el set de test\ntest_preds = best_model.predict(X_test)          # predicciones 0/1\ntest_preds = test_preds.astype(int)              # nos aseguramos de tener enteros\n\n# 3) Armamos el DataFrame de submission en el formato oficial\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_passenger_id,           # IDs del test\n    \"Survived\": test_preds                      # predicciones binarias\n})\n\n# 4) Chequeos de formato (defensivos)\nassert submission.shape[0] == 418, \"El submission debe tener exactamente 418 filas.\"\nassert list(submission.columns) == [\"PassengerId\", \"Survived\"], \"Columnas deben ser PassengerId y Survived.\"\n\n# 5) Guardamos el CSV\nsubmission.to_csv(\"submission.csv\", index=False)\n\n# 6) Confirmaci√≥n y vista previa\nprint(\"‚úÖ Archivo 'submission.csv' creado con\", submission.shape[0], \"filas.\")\nprint(submission.head())\nprint(\"\\nConteo predicciones:\", submission[\"Survived\"].value_counts().to_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T16:52:09.875531Z","iopub.execute_input":"2025-08-29T16:52:09.876334Z","iopub.status.idle":"2025-08-29T16:52:09.910769Z","shell.execute_reply.started":"2025-08-29T16:52:09.876299Z","shell.execute_reply":"2025-08-29T16:52:09.909828Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Archivo 'submission.csv' creado con 418 filas.\n   PassengerId  Survived\n0          892         0\n1          893         1\n2          894         0\n3          895         0\n4          896         1\n\nConteo predicciones: {0: 252, 1: 166}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# üèÜ Celda 9 ‚Äî Registro del resultado\n\n- **Score Kaggle:** 0.77033  \n- **Notebook:** EPA_TITANIC_LOGREG_V2.ipynb  \n- **Modelo:** Regresi√≥n Log√≠stica con GridSearchCV  \n- **Mejores hiperpar√°metros (CV):** C‚âà7.74, penalty=l2, solver=lbfgs  \n- **Notas:**  \n  - CV promedio (5 folds): 0.8272  \n  - Score en Kaggle fue **ligeramente peor** que V1 (0.77272).  \n  - Probable **sobreajuste por baja regularizaci√≥n** (C alto).  \n  - Pr√≥ximo paso: probar modelos basados en √°rboles (Random Forest, Gradient Boosting) con estas features.  \n","metadata":{}}]}