{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üö¢ EPA Titanic ‚Äî SGDClassifier V1 (Modelos lineales estoc√°sticos)\n\nEn este notebook probaremos el **SGDClassifier** de scikit-learn aplicado al cl√°sico problema del Titanic.  \n\n## üîé ¬øQu√© es el SGDClassifier?\n\nEl **SGDClassifier** es un clasificador lineal que entrena el modelo mediante **descenso de gradiente estoc√°stico (SGD, Stochastic Gradient Descent)**.  \nA diferencia de `LogisticRegression` o `LinearSVC`, que usan optimizadores m√°s \"exactos\", este modelo ajusta los par√°metros en forma **iterativa y aproximada**, lo que lo hace:\n\n- ‚ö° **Muy eficiente** en datasets grandes y dispersos.  \n- üîÑ Compatible con distintos tipos de funciones de p√©rdida (`loss`), lo que le da flexibilidad:  \n  - `loss=\"log_loss\"` ‚Üí equivalente a regresi√≥n log√≠stica.  \n  - `loss=\"hinge\"` ‚Üí equivalente a SVM lineal.  \n  - `loss=\"modified_huber\"`, `perceptron`, etc.  \n- üéõÔ∏è Permite ajustar la regularizaci√≥n: `penalty = l1`, `l2`, o `elasticnet`.  \n- üìâ Actualiza los par√°metros en **mini-batches** de datos, lo que reduce el costo computacional y puede mejorar la generalizaci√≥n.\n\n## üéØ Objetivo del notebook\n1. Reutilizar el **Feature Engineering** y el **preprocesamiento** ya definidos en experimentos anteriores.  \n2. Entrenar un pipeline con `SGDClassifier`.  \n3. Evaluar con validaci√≥n cruzada (5-fold stratified).  \n4. Probar distintas configuraciones de `loss` y `penalty` para ver cu√°l generaliza mejor en Titanic.  \n5. Generar un `submission.csv` y registrar el resultado.  \n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T02:50:18.353928Z","iopub.execute_input":"2025-08-30T02:50:18.354258Z","iopub.status.idle":"2025-08-30T02:50:20.719996Z","shell.execute_reply.started":"2025-08-30T02:50:18.354224Z","shell.execute_reply":"2025-08-30T02:50:20.719208Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# üì• Celda 2 ‚Äî Carga de datos\nEn esta celda vamos a:  \n- Importar las librer√≠as necesarias (`pandas` para manipulaci√≥n tabular).  \n- Leer los datasets oficiales de Kaggle (`train.csv` y `test.csv`) desde la carpeta `/kaggle/input/titanic/`.  \n- Verificar las dimensiones de ambos datasets para asegurarnos de que tengan el tama√±o esperado:  \n  - `train.csv` ‚Üí 891 filas, 12 columnas (incluye la variable objetivo `Survived`).  \n  - `test.csv` ‚Üí 418 filas, 11 columnas (sin `Survived`, usado para predicci√≥n).  \n- Mostrar las primeras filas del dataset de entrenamiento para inspeccionar la estructura y tipos de variables.  \n","metadata":{}},{"cell_type":"code","source":"# Importamos pandas para trabajar con datos en formato tabla (DataFrame)\nimport pandas as pd  \n\n# Leemos el dataset de entrenamiento, que incluye la columna objetivo \"Survived\"\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")  \n\n# Leemos el dataset de test, que no incluye \"Survived\" (lo vamos a predecir)\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")   \n\n# Imprimimos la forma (filas, columnas) del train para validar tama√±o esperado (891, 12)\nprint(\"Shape train:\", train_df.shape)  \n\n# Imprimimos la forma del test para validar tama√±o esperado (418, 11)\nprint(\"Shape test :\", test_df.shape)   \n\n# Mostramos las primeras 5 filas del train para visualizar estructura, columnas y tipos de datos\ntrain_df.head()  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:05:45.835818Z","iopub.execute_input":"2025-08-30T03:05:45.836196Z","iopub.status.idle":"2025-08-30T03:05:45.896830Z","shell.execute_reply.started":"2025-08-30T03:05:45.836170Z","shell.execute_reply":"2025-08-30T03:05:45.895941Z"}},"outputs":[{"name":"stdout","text":"Shape train: (891, 12)\nShape test : (418, 11)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# üîé Celda 3 ‚Äî Exploraci√≥n inicial (EDA breve)\nEn esta celda vamos a:  \n- Listar todas las columnas presentes en el dataset de entrenamiento.  \n- Revisar valores faltantes en `train.csv` y `test.csv` (conteo y porcentaje).  \n- Analizar la distribuci√≥n de la variable objetivo `Survived` para ver el balance de clases.  \n\nEsto nos permitir√° identificar qu√© variables necesitan imputaci√≥n y verificar si hay desbalance que pueda afectar al entrenamiento del modelo.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np  # librer√≠a para c√°lculos num√©ricos\n\n# --- Listado de columnas ---\nprint(\"Columnas en train:\", train_df.columns.tolist())  # muestra todas las columnas de train\n\n# --- Faltantes en TRAIN ---\nna_train = train_df.isna().sum()                          # cuenta los valores NaN por columna\nna_train_pct = (na_train / len(train_df)).round(3)        # porcentaje de NaN por columna\nfaltantes_train = pd.DataFrame({\"nulos\": na_train, \"porcentaje\": na_train_pct})  # tabla resumen\nprint(\"\\nFaltantes en train (conteo y %):\")\nprint(faltantes_train.sort_values(\"nulos\", ascending=False))  # ordena de mayor a menor\n\n# --- Faltantes en TEST ---\nna_test = test_df.isna().sum()                            # cuenta los NaN en test\nna_test_pct = (na_test / len(test_df)).round(3)           # porcentaje en test\nfaltantes_test = pd.DataFrame({\"nulos\": na_test, \"porcentaje\": na_test_pct})  # tabla resumen\nprint(\"\\nFaltantes en test (conteo y %):\")\nprint(faltantes_test.sort_values(\"nulos\", ascending=False))\n\n# --- Distribuci√≥n de la variable objetivo (Survived) ---\ny_counts = train_df[\"Survived\"].value_counts().sort_index()               # conteo por clase (0 y 1)\ny_ratio = train_df[\"Survived\"].value_counts(normalize=True).round(4)      # proporci√≥n relativa\nprint(\"\\nDistribuci√≥n de Survived (conteo):\")\nprint(y_counts)\nprint(\"\\nDistribuci√≥n de Survived (proporci√≥n):\")\nprint(y_ratio)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:12:06.515640Z","iopub.execute_input":"2025-08-30T03:12:06.516551Z","iopub.status.idle":"2025-08-30T03:12:06.545249Z","shell.execute_reply.started":"2025-08-30T03:12:06.516522Z","shell.execute_reply":"2025-08-30T03:12:06.544345Z"}},"outputs":[{"name":"stdout","text":"Columnas en train: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\nFaltantes en train (conteo y %):\n             nulos  porcentaje\nCabin          687       0.771\nAge            177       0.199\nEmbarked         2       0.002\nPassengerId      0       0.000\nSurvived         0       0.000\nPclass           0       0.000\nName             0       0.000\nSex              0       0.000\nSibSp            0       0.000\nParch            0       0.000\nTicket           0       0.000\nFare             0       0.000\n\nFaltantes en test (conteo y %):\n             nulos  porcentaje\nCabin          327       0.782\nAge             86       0.206\nFare             1       0.002\nPassengerId      0       0.000\nPclass           0       0.000\nName             0       0.000\nSex              0       0.000\nSibSp            0       0.000\nParch            0       0.000\nTicket           0       0.000\nEmbarked         0       0.000\n\nDistribuci√≥n de Survived (conteo):\nSurvived\n0    549\n1    342\nName: count, dtype: int64\n\nDistribuci√≥n de Survived (proporci√≥n):\nSurvived\n0    0.6162\n1    0.3838\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# üß± Celda 4 ‚Äî Definici√≥n de variables base y objetivo\nEn esta celda vamos a:  \n- Seleccionar las columnas **num√©ricas** y **categ√≥ricas** originales que servir√°n como punto de partida.  \n- Construir la matriz de features `X` (entrenamiento) y `X_test` (predicci√≥n).  \n- Definir la variable objetivo `y` (`Survived`).  \n- Guardar `PassengerId` del test para generar el `submission.csv` al final.\n","metadata":{}},{"cell_type":"code","source":"# --- Definici√≥n de columnas base ---\n# Variables num√©ricas originales del dataset\nbase_num_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass\"]\n\n# Variables categ√≥ricas originales del dataset\nbase_cat_features = [\"Sex\", \"Embarked\"]\n\n# --- Construcci√≥n de X (features de entrenamiento) ---\nX = train_df[base_num_features + base_cat_features].copy()  # seleccionamos columnas base del train\n\n# --- Variable objetivo (target) ---\ny = train_df[\"Survived\"].astype(int)  # convertimos Survived a entero (0/1)\n\n# --- Construcci√≥n de X_test (features de predicci√≥n) ---\nX_test = test_df[base_num_features + base_cat_features].copy()  # seleccionamos columnas base del test\n\n# --- Guardamos PassengerId (para submission) ---\ntest_passenger_id = test_df[\"PassengerId\"].copy()  # IDs del test\n\n# Vista previa de X\nX.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:19:26.428530Z","iopub.execute_input":"2025-08-30T03:19:26.428848Z","iopub.status.idle":"2025-08-30T03:19:26.446610Z","shell.execute_reply.started":"2025-08-30T03:19:26.428823Z","shell.execute_reply":"2025-08-30T03:19:26.445831Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    Age  SibSp  Parch     Fare  Pclass     Sex Embarked\n0  22.0      1      0   7.2500       3    male        S\n1  38.0      1      0  71.2833       1  female        C\n2  26.0      0      0   7.9250       3  female        S\n3  35.0      1      0  53.1000       1  female        S\n4  35.0      0      0   8.0500       3    male        S","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>3</td>\n      <td>male</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>female</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>3</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>1</td>\n      <td>female</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>3</td>\n      <td>male</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# üß™ Celda 5 ‚Äî Ingenier√≠a de atributos\nEn esta celda a√±adiremos nuevas variables derivadas a los datos:  \n\n- `FamilySize = SibSp + Parch + 1`  \n- `IsAlone = 1 si FamilySize == 1, en caso contrario 0`  \n- `Title` (extra√≠do de `Name`, normalizando y agrupando raros en `Other`)  \n- `CabinKnown = 1 si la variable Cabin no es NaN`  \n- `FarePerPerson = Fare / FamilySize`  \n- `TicketGroupSize = n√∫mero de pasajeros que comparten el mismo Ticket`  \n\nImplementaremos estas transformaciones dentro de una funci√≥n y la integraremos al pipeline usando `FunctionTransformer`.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer  # para aplicar funciones custom en un Pipeline\n\n# Definimos la funci√≥n que crear√° las nuevas features\ndef add_features(df):\n    df = df.copy()  # copiamos para no modificar el DataFrame original\n\n    # --- FamilySize e IsAlone ---\n    df[\"FamilySize\"] = df[\"SibSp\"].fillna(0) + df[\"Parch\"].fillna(0) + 1\n    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n\n    # --- Title (extra√≠do de Name) ---\n    if \"Name\" in df.columns:\n        titles = df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\")[0]\n        titles = titles.replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n        titles = titles.where(titles.isin([\"Mr\",\"Mrs\",\"Miss\",\"Master\"]), \"Other\")\n        df[\"Title\"] = titles.fillna(\"Other\")\n    else:\n        df[\"Title\"] = \"Other\"\n\n    # --- CabinKnown ---\n    if \"Cabin\" in df.columns:\n        df[\"CabinKnown\"] = (~df[\"Cabin\"].isna()).astype(int)\n    else:\n        df[\"CabinKnown\"] = 0\n\n    # --- FarePerPerson ---\n    df[\"FarePerPerson\"] = df[\"Fare\"].fillna(df[\"Fare\"].median()) / df[\"FamilySize\"].replace(0, 1)\n\n    # --- TicketGroupSize ---\n    if \"Ticket\" in df.columns:\n        counts = df[\"Ticket\"].map(df[\"Ticket\"].value_counts())\n        df[\"TicketGroupSize\"] = counts.fillna(1).astype(int)\n    else:\n        df[\"TicketGroupSize\"] = 1\n\n    return df\n\n# Creamos el transformador que usaremos en el Pipeline\nfeature_engineering = FunctionTransformer(add_features, validate=False)\n\n# A√±adimos columnas auxiliares a X y X_test necesarias para la ingenier√≠a de atributos\nfor col in [\"Name\", \"Ticket\", \"Cabin\"]:\n    if col not in X.columns and col in train_df.columns:\n        X[col] = train_df[col]\n    if col not in X_test.columns and col in test_df.columns:\n        X_test[col] = test_df[col]\n\nprint(\"‚úÖ Ingenier√≠a de atributos lista (FunctionTransformer definido y columnas auxiliares a√±adidas).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:26:02.124291Z","iopub.execute_input":"2025-08-30T03:26:02.125099Z","iopub.status.idle":"2025-08-30T03:26:02.745529Z","shell.execute_reply.started":"2025-08-30T03:26:02.125073Z","shell.execute_reply":"2025-08-30T03:26:02.744820Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Ingenier√≠a de atributos lista (FunctionTransformer definido y columnas auxiliares a√±adidas).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# üßΩ Celda 6 ‚Äî Preprocesamiento (imputaci√≥n, escalado y One-Hot)\nEn esta celda definimos un `ColumnTransformer` que:\n- Imputa **num√©ricas** con **mediana** y luego **escala** (SGD es sensible a magnitudes).\n- Imputa **categ√≥ricas** con **moda** y aplica **One-Hot** (incluye `Title` creado en la Celda 5).\nSolo listamos las **features finales** (las columnas auxiliares `Name`, `Ticket`, `Cabin` no se usan como entrada del modelo).\n","metadata":{}},{"cell_type":"code","source":"# Importamos utilidades para construir el preprocesamiento por tipo de columna\nfrom sklearn.compose import ColumnTransformer            # permite aplicar distintos transformadores por columna\nfrom sklearn.pipeline import Pipeline                    # para encadenar pasos (FE -> pre -> modelo)\nfrom sklearn.impute import SimpleImputer                 # imputaci√≥n de valores faltantes\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder  # escalado y one-hot\n\n# Definimos las columnas que EXISTIR√ÅN DESPU√âS del Feature Engineering (Celda 5)\nnum_features = [                                         # variables num√©ricas finales\n    \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass\",           # num√©ricas originales\n    \"FamilySize\", \"IsAlone\", \"FarePerPerson\",            # features creadas\n    \"TicketGroupSize\", \"CabinKnown\"                      # features creadas (CabinKnown es binaria pero tratamos como num)\n]\ncat_features = [                                         # variables categ√≥ricas finales\n    \"Sex\", \"Embarked\", \"Title\"                           # Title proviene del FE\n]\n\n# Pipeline para num√©ricas: imputar con mediana y escalar (muy importante para SGD)\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),       # rellena NaN con la mediana (robusto a outliers)\n    (\"scaler\", StandardScaler())                         # estandariza (media 0, desv√≠o 1) para mejorar la optimizaci√≥n\n])\n\n# Pipeline para categ√≥ricas: imputar con moda y codificar con One-Hot\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),             # rellena NaN con el valor m√°s frecuente\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # convierte a dummies densos\n])\n\n# Componemos el ColumnTransformer que aplicar√° cada pipeline a su grupo de columnas\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, num_features),      # aplica el pipeline num√©rico a num_features\n        (\"cat\", categorical_transformer, cat_features)   # aplica el pipeline categ√≥rico a cat_features\n    ],\n    remainder=\"drop\"                                     # descarta columnas no listadas (Name/Ticket/Cabin no pasan)\n)\n\n# Mensaje de control para verificar configuraci√≥n\nprint(\"‚úÖ Preprocesamiento definido para SGD.\")\nprint(\"Num features:\", num_features)\nprint(\"Cat features:\", cat_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:38:32.904626Z","iopub.execute_input":"2025-08-30T03:38:32.905465Z","iopub.status.idle":"2025-08-30T03:38:33.266133Z","shell.execute_reply.started":"2025-08-30T03:38:32.905436Z","shell.execute_reply":"2025-08-30T03:38:33.265227Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Preprocesamiento definido para SGD.\nNum features: ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass', 'FamilySize', 'IsAlone', 'FarePerPerson', 'TicketGroupSize', 'CabinKnown']\nCat features: ['Sex', 'Embarked', 'Title']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# ü§ñ Celda 7 ‚Äî Pipeline completo (FE ‚Üí Preprocesamiento ‚Üí SGDClassifier) + CV\nEntrenamos un **SGDClassifier** (modelo lineal entrenado con descenso de gradiente estoc√°stico) usando:\n- `loss=\"log_loss\"` (equivalente a regresi√≥n log√≠stica),\n- `penalty=\"elasticnet\"` (mezcla de L1/L2),  \n- validaci√≥n cruzada **estratificada** de 5 folds con m√©trica *accuracy*.\n\n> Nota: Este es un baseline razonable para SGD. En la pr√≥xima celda haremos un **GridSearch** con varias combinaciones (`loss`, `alpha`, `penalty`, `l1_ratio`).\n","metadata":{}},{"cell_type":"code","source":"# Importamos el clasificador y utilidades de validaci√≥n\nfrom sklearn.linear_model import SGDClassifier              # modelo lineal con SGD\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score  # CV estratificada y evaluaci√≥n\nfrom sklearn.pipeline import Pipeline                       # para encadenar pasos\nimport numpy as np                                          # utilidades num√©ricas\n\n# Definimos un SGDClassifier \"baseline\" con configuraci√≥n estable\nsgd = SGDClassifier(\n    loss=\"log_loss\",     # p√©rdida tipo regresi√≥n log√≠stica (probabil√≠stica)\n    penalty=\"elasticnet\",# regularizaci√≥n combinada L1+L2 (controlada por l1_ratio)\n    alpha=5e-4,          # fuerza de regularizaci√≥n (m√°s alto = m√°s regularizaci√≥n)\n    l1_ratio=0.15,       # proporci√≥n de L1 dentro de elasticnet (0 = L2 puro, 1 = L1 puro)\n    max_iter=2000,       # √©pocas m√°ximas (iteraciones) para converger\n    tol=1e-3,            # criterio de parada temprana (si mejora < tol)\n    random_state=42      # reproducibilidad\n    # class_weight=\"balanced\"  # <- opcional si quisieras compensar el leve desbalance\n)\n\n# Armamos el pipeline completo: FE -> Preprocesamiento -> Modelo\npipe_sgd = Pipeline(steps=[\n    (\"fe\", feature_engineering),  # agrega features creadas en la Celda 5\n    (\"pre\", preprocessor),        # imputaci√≥n + escalado + one-hot\n    (\"sgd\", sgd)                  # clasificador lineal con SGD\n])\n\n# Definimos validaci√≥n cruzada estratificada (mantiene proporci√≥n de clases 0/1 en cada fold)\ncv = StratifiedKFold(\n    n_splits=5,    # n√∫mero de folds\n    shuffle=True,  # mezclamos antes de partir\n    random_state=42\n)\n\n# Ejecutamos cross-validation con accuracy (m√©trica oficial de la competencia)\ncv_scores = cross_val_score(\n    estimator=pipe_sgd,  # pipeline completo\n    X=X,                 # features base (FE se aplica dentro del pipeline)\n    y=y,                 # objetivo (Survived)\n    cv=cv,               # esquema de validaci√≥n\n    scoring=\"accuracy\",  # m√©trica de evaluaci√≥n\n    n_jobs=-1            # usa todos los n√∫cleos disponibles\n)\n\n# Mostramos resultados por fold y resumen\nprint(\"Accuracy por fold:\", np.round(cv_scores, 4))                 # muestra accuracy de cada fold\nprint(\"Accuracy promedio (CV):\", cv_scores.mean().round(4),         # promedio\n      \"| Desv. std:\", cv_scores.std().round(4))                     # dispersi√≥n entre folds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:41:38.611366Z","iopub.execute_input":"2025-08-30T03:41:38.611681Z","iopub.status.idle":"2025-08-30T03:41:40.729729Z","shell.execute_reply.started":"2025-08-30T03:41:38.611659Z","shell.execute_reply":"2025-08-30T03:41:40.728893Z"}},"outputs":[{"name":"stdout","text":"Accuracy por fold: [0.8268 0.8146 0.7809 0.8202 0.8483]\nAccuracy promedio (CV): 0.8182 | Desv. std: 0.0219\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# üß™ Celda 8 ‚Äî GridSearchCV para SGDClassifier\nBuscamos la mejor combinaci√≥n de hiperpar√°metros para **SGDClassifier**.  \nExploraremos:\n- `loss` ‚Üí `\"log_loss\"` (log√≠stica), `\"hinge\"` (SVM lineal), `\"modified_huber\"` (robusta).  \n- `penalty` ‚Üí `\"l2\"` (estable) y `\"elasticnet\"` (mezcla L1/L2).  \n- `alpha` ‚Üí fuerza de regularizaci√≥n (m√°s alto = m√°s regularizaci√≥n).  \n- `l1_ratio` ‚Üí proporci√≥n L1 dentro de `elasticnet` (solo aplica si `penalty=\"elasticnet\"`).\n\nUsamos **early stopping** dentro del estimador para estabilizar el entrenamiento estoc√°stico.\n","metadata":{}},{"cell_type":"code","source":"# Importamos utilidades de b√∫squeda y validaci√≥n\nfrom sklearn.linear_model import SGDClassifier                    # clasificador lineal con SGD\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV # validaci√≥n cruzada y grid search\nfrom sklearn.pipeline import Pipeline                             # para definir el pipeline completo\nimport numpy as np                                                # para listas de hiperpar√°metros\nimport pandas as pd                                               # para mostrar resultados ordenados\n\n# 1) Definimos un SGDClassifier base (los hiperpar√°metros variar√°n en la grilla)\nsgd_base = SGDClassifier(\n    max_iter=5000,          # m√°s iteraciones para asegurar convergencia\n    tol=1e-3,               # criterio de parada\n    early_stopping=True,    # activa parada temprana con un hold-out interno\n    validation_fraction=0.1,# 10% del fold para validaci√≥n interna del early stopping\n    n_iter_no_change=5,     # paciencia para early stopping\n    random_state=42,        # reproducibilidad\n    # note: class_weight no est√° disponible en SGDClassifier\n)\n\n# 2) Definimos el pipeline completo: FE -> Preprocesamiento -> SGD\npipe_sgd = Pipeline(steps=[\n    (\"fe\", feature_engineering),   # crea features (FamilySize, Title, etc.)\n    (\"pre\", preprocessor),         # imputaci√≥n + escalado + one-hot\n    (\"sgd\", sgd_base)              # modelo a tunear\n])\n\n# 3) Definimos la grilla de hiperpar√°metros\nalpha_values = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2]    # regularizaci√≥n en escala log\nl1_values    = [0.15, 0.3, 0.5]                  # solo aplican si penalty='elasticnet'\n\nparam_grid = [\n    # penalty L2 (m√°s estable)\n    {\n        \"sgd__loss\": [\"log_loss\", \"modified_huber\", \"hinge\"],\n        \"sgd__penalty\": [\"l2\"],\n        \"sgd__alpha\": alpha_values,\n    },\n    # penalty ElasticNet (mezcla L1/L2)\n    {\n        \"sgd__loss\": [\"log_loss\", \"modified_huber\", \"hinge\"],\n        \"sgd__penalty\": [\"elasticnet\"],\n        \"sgd__alpha\": alpha_values,\n        \"sgd__l1_ratio\": l1_values,\n    },\n]\n\n# 4) Esquema de validaci√≥n cruzada estratificada (mantiene proporci√≥n 0/1)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 5) Configuramos y ejecutamos el GridSearchCV\ngrid = GridSearchCV(\n    estimator=pipe_sgd,        # pipeline completo\n    param_grid=param_grid,     # combinaciones a explorar\n    scoring=\"accuracy\",        # m√©trica oficial de la competici√≥n\n    cv=cv,                     # validaci√≥n cruzada\n    n_jobs=-1,                 # usa todos los cores disponibles\n    refit=True,                # reentrena el mejor modelo en todo el train\n    verbose=1                  # nivel de verbosidad\n)\n\ngrid.fit(X, y)                 # entrena y eval√∫a todas las combinaciones\n\n# 6) Reportamos el mejor resultado\nprint(\"üîù Mejor accuracy (CV):\", grid.best_score_.round(4))\nprint(\"üîß Mejores hiperpar√°metros:\", grid.best_params_)\n\n# Guardamos el mejor pipeline ya ajustado\nbest_model = grid.best_estimator_\n\n# 7) (Opcional) Mostramos el Top-5 de resultados ordenados por mean_test_score\nres = pd.DataFrame(grid.cv_results_).sort_values(\"mean_test_score\", ascending=False)\ncols = [\n    \"mean_test_score\",\"std_test_score\",\n    \"param_sgd__loss\",\"param_sgd__penalty\",\"param_sgd__alpha\",\"param_sgd__l1_ratio\"\n]\ndisplay(res[cols].head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:46:16.677679Z","iopub.execute_input":"2025-08-30T03:46:16.678048Z","iopub.status.idle":"2025-08-30T03:46:21.221111Z","shell.execute_reply.started":"2025-08-30T03:46:16.678023Z","shell.execute_reply":"2025-08-30T03:46:21.220228Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 60 candidates, totalling 300 fits\nüîù Mejor accuracy (CV): 0.8215\nüîß Mejores hiperpar√°metros: {'sgd__alpha': 0.01, 'sgd__loss': 'hinge', 'sgd__penalty': 'l2'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    mean_test_score  std_test_score param_sgd__loss param_sgd__penalty  \\\n14         0.821537        0.013158           hinge                 l2   \n12         0.821537        0.007714        log_loss                 l2   \n9          0.819302        0.012022        log_loss                 l2   \n42         0.819296        0.004522        log_loss         elasticnet   \n53         0.818178        0.008471           hinge         elasticnet   \n\n   param_sgd__alpha param_sgd__l1_ratio  \n14             0.01                 NaN  \n12             0.01                 NaN  \n9             0.003                 NaN  \n42            0.003                0.15  \n53             0.01                0.15  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>param_sgd__loss</th>\n      <th>param_sgd__penalty</th>\n      <th>param_sgd__alpha</th>\n      <th>param_sgd__l1_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14</th>\n      <td>0.821537</td>\n      <td>0.013158</td>\n      <td>hinge</td>\n      <td>l2</td>\n      <td>0.01</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.821537</td>\n      <td>0.007714</td>\n      <td>log_loss</td>\n      <td>l2</td>\n      <td>0.01</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.819302</td>\n      <td>0.012022</td>\n      <td>log_loss</td>\n      <td>l2</td>\n      <td>0.003</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.819296</td>\n      <td>0.004522</td>\n      <td>log_loss</td>\n      <td>elasticnet</td>\n      <td>0.003</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>0.818178</td>\n      <td>0.008471</td>\n      <td>hinge</td>\n      <td>elasticnet</td>\n      <td>0.01</td>\n      <td>0.15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# üì§ Celda 9 ‚Äî Entrenamiento final y creaci√≥n de `submission.csv`\nUsamos el **mejor pipeline del GridSearch** (`best_model`) para predecir en `test` y\nguardamos el archivo `submission.csv` con el formato requerido por Kaggle:\n\n- Columnas: `PassengerId`, `Survived`\n- Filas: 418 (una por cada pasajero del set de test)\n","metadata":{}},{"cell_type":"code","source":"# 1) Aseg√∫rate de haber ejecutado la Celda 8 (GridSearch) para tener `best_model` en memoria.\n#    `best_model` ya est√° reentrenado con refit=True sobre TODO el train del fold (pipeline completo).\n\n# 2) Predicci√≥n en el set de test\ntest_preds = best_model.predict(X_test)      # obtenemos predicciones binarias 0/1\ntest_preds = test_preds.astype(int)          # convertimos expl√≠citamente a enteros\n\n# 3) Construcci√≥n del DataFrame de submission\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_passenger_id,        # IDs del set de test\n    \"Survived\": test_preds                   # predicciones del mejor modelo\n})\n\n# 4) Chequeos de formato (defensivos)\nassert submission.shape[0] == 418, \"El submission debe tener exactamente 418 filas.\"\nassert list(submission.columns) == [\"PassengerId\", \"Survived\"], \"Las columnas deben ser PassengerId y Survived.\"\n\n# 5) Guardamos el CSV final para subir a Kaggle\nsubmission.to_csv(\"submission.csv\", index=False)\n\n# 6) Confirmaci√≥n y vista previa\nprint(\"‚úÖ Archivo 'submission.csv' creado con\", submission.shape[0], \"filas.\")\nprint(submission.head())\nprint(\"\\nConteo predicciones:\", submission['Survived'].value_counts().to_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:51:18.770800Z","iopub.execute_input":"2025-08-30T03:51:18.771128Z","iopub.status.idle":"2025-08-30T03:51:18.813017Z","shell.execute_reply.started":"2025-08-30T03:51:18.771108Z","shell.execute_reply":"2025-08-30T03:51:18.812294Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Archivo 'submission.csv' creado con 418 filas.\n   PassengerId  Survived\n0          892         0\n1          893         1\n2          894         0\n3          895         0\n4          896         1\n\nConteo predicciones: {0: 256, 1: 162}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# üèÜ Celda 10 ‚Äî Registro del resultado\n\n- **Score Kaggle:** 0.77511  \n- **Notebook:** EPA_TITANIC_SGD_V1.ipynb  \n- **Modelo:** SGDClassifier (SVM lineal con descenso de gradiente estoc√°stico)  \n- **Mejores hiperpar√°metros (CV):**  \n  - loss = \"hinge\"  \n  - penalty = \"l2\"  \n  - alpha = 0.01  \n- **Resultados:**  \n  - CV promedio: 0.8215  \n  - Kaggle score: 0.77511 (**mejor modelo hasta ahora**)  \n- **Notas:**  \n  - La regularizaci√≥n relativamente fuerte (`alpha=0.01`) estabiliz√≥ el modelo.  \n  - El modelo replica muy bien la proporci√≥n real de clases.  \n  - Pr√≥ximos pasos: probar ensambles (Random Forest, Gradient Boosting) para intentar superar 0.78+.  \n","metadata":{}}]}